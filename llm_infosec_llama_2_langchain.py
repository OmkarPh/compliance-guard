# -*- coding: utf-8 -*-
"""llm_infosec_llama_2_langchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zQl2gxF1Nv9QFwOS0vRHWItCc6CGTLJ2
"""

BASE_DIR = "/content/gdrive/MyDrive/outfit-genai"
KNOWLEDGE_BASE_DIR = f"{BASE_DIR}/llm/infosec_knowledge_base"
ENV_FILE_PATH = f"{BASE_DIR}/.env"

from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

!nvidia-smi

#@markdown # Connect Google Drive
from google.colab import drive
from IPython.display import clear_output
import ipywidgets as widgets
import os

def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)
Shared_Drive = "" #@param {type:"string"}
#@markdown - Leave empty if you're not using a shared drive

print("[0;33mConnecting...")
drive.mount('/content/gdrive')

if Shared_Drive!="" and os.path.exists("/content/gdrive/Shareddrives"):
  mainpth="Shareddrives/"+Shared_Drive
else:
  mainpth="MyDrive"

clear_output()
inf('\u2714 Done','success', '50px')

#@markdown ---

!pip install -Uqqq pip --progress-bar off
!pip install -qqq python-dotenv --progress-bar off
!pip install -qqq jq fastapi nest-asyncio pyngrok uvicorn --progress-bar off
!pip install -qqq torch==2.0.1 --progress-bar off
!pip install -qqq transformers==4.31.0 --progress-bar off
!pip install -qqq langchain==0.0.266 --progress-bar off
!pip install -qqq chromadb==0.4.5 --progress-bar off
!pip install -qqq pypdf==3.15.0 --progress-bar off
!pip install -qqq xformers==0.0.20 --progress-bar off
!pip install -qqq sentence_transformers==2.2.2 --progress-bar off
!pip install -qqq InstructorEmbedding==1.0.1 --progress-bar off
!pip install -qqq pdf2image==1.16.3 --progress-bar off
!pip install -qqq "unstructured[all-docs]" --progress-bar off
!pip install -qqq $BASE_DIR/llm/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off

!sudo apt-get install poppler-utils

!pip install -qqq "unstructured[all-docs]"

import torch
import re
import json
import os
import dotenv
import pandas as pd
from auto_gptq import AutoGPTQForCausalLM
from langchain import HuggingFacePipeline, PromptTemplate
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFDirectoryLoader, JSONLoader, DirectoryLoader, TextLoader, CSVLoader, PyPDFLoader
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from pdf2image import convert_from_path
from transformers import AutoTokenizer, TextStreamer, pipeline, AutoConfig

# Configuration
dotenv.load_dotenv(ENV_FILE_PATH)
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
Ngrok_token = os.environ.get('LLM_NGROK_TOKEN')
Ngrok_domain = os.environ.get('LLM_NGROK_DOMAIN')

"""## Data"""

# Load knowledge base documents
!rm -rf "db"
documents = []

txtLoader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob="**/*.txt")
txtDocs = txtLoader.load()
csvLoader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob="**/*.csv")
csvDocs = csvLoader.load()
pdfLoader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob="**/*.pdf")
pdfDocs = pdfLoader.load()

def flatten(l):
    return [item for sublist in l for item in sublist]
documents = flatten([txtDocs, csvDocs, pdfDocs])
print(f"Loaded {len(documents)} documents")

embeddings = HuggingFaceInstructEmbeddings(
    model_name="hkunlp/instructor-large", model_kwargs={"device": DEVICE}
)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
texts = text_splitter.split_documents(documents)
print("Knowledge text length", len(texts))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# db = Chroma.from_documents(texts, embeddings, persist_directory="db")

"""## Llama 2 13B"""

# model_name_or_path = "l"
quantized_model_dir = f"{BASE_DIR}/llm/models/gptq_model"
model_basename = "gptq_model-4bit-128g"

tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)

model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    revision="gptq-4bit-128g-actorder_True",
    model_basename=model_basename,
    use_safetensors=True,
    trust_remote_code=True,
    inject_fused_attention=False,
    device=DEVICE,
    quantize_config=None,
)

!nvidia-smi

# DEFAULT_SYSTEM_PROMPT = """
# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

# If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
# """.strip()
DEFAULT_SYSTEM_PROMPT = """
You are a helpful, respectful and honest security compliance assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are technically correct.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
""".strip()


def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:
    return f"""
[INST] <<SYS>>
{system_prompt}
<</SYS>>

{prompt} [/INST]
""".strip()

streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

text_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1024,
    temperature=0,
    top_p=0.95,
    repetition_penalty=1.15,
    streamer=streamer,
)

llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={"temperature": 0})

SYSTEM_PROMPT = "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Please don't include prefix like based on provided information, etc"

template = generate_prompt(
    """
{context}

Question: {question}
""",
    system_prompt=SYSTEM_PROMPT,
)

prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=db.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)

def check_compliance_status(input_string):
    # Convert the input string to lowercase for case-insensitive matching
    lower_input = input_string.lower()

    # Check if the input string contains the compliant or non-compliant phrases
    is_compliant = "the log is compliant" in lower_input
    is_not_compliant = "the log is not compliant" in lower_input

    # Determine the compliance status based on the presence of the phrases
    if is_compliant:
        return True
    elif is_not_compliant:
        return False
    return False

def extract_compliance_info(text: str):
    keys_mapping = {
        "Descriptive cause of non compliance of logs": "cause",
        "Exact compliance breaches information": "breaches",
        "Actionable Insights on the issue that may occur due to this log": "insight",
        "Possible teams to communicate regarding the issue": "teams"
    }

    result = {}

    for key, value in keys_mapping.items():
        match = re.search(re.escape(key) + r'\s*:\s*([\s\S]+?)(?=\n\S+|$)', text)
        if match:
            result[value] = match.group(1).strip()
    return result

def analyseLog(logText: str):
    prompt = f"""
As an assistant with a focus on security compliance and log monitoring for an ecommerce platform, Check if the following log is compliant in context of the rulset i have provided (Don't focus on inconsistent log formats)
Log: `{logText}` If the log is compliant with security policies, strictly start your response with this hardcoded string: `The log is compliant`
If the log is not compliant, strictly start your response with this hardcoded string: `The log is not compliant`


If the log is not compliant, give me insights on the log with following points as prefix:
1. Descriptive cause of non compliance of logs:
2. Exact compliance breaches information:
3. Actionable Insights on the issue that may occur due to this log:
4. Possible teams to communicate regarding the issue:
"""

    analysis = qa_chain(prompt)
    print(analysis)

    answer = analysis['result']

    return {
        'isCompliant': check_compliance_status(answer),
        'report': extract_compliance_info(answer),
        'log': logText,
        'answer': answer
    }

# extract_compliance_info("""The log is not compliant.
# 1. Descriptive cause of non compliance of logs: Fraudulent Payment Attempt.
# 2. Exact compliance breaches information: TransactionID: 234.
# 3. Actionable Insights on the issue that may occur due to this log: This log indicates a potential fraudulent payment attempt, which could result in financial loss or damage to the platform's reputation.
# 4. Possible teams to communicate regarding the issue: Security Team, Risk Management Team, Legal Team.""")

# analyseLog("2023-09-13 14:50:22,WARN,PaymentService,PaymentModule,UserID505,Fraudulent Payment Attempt,TransactionID: 234")

"""## Chat apis"""

from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()
origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
def read_root():
    return {"Message": "Hello World !"}

@app.get("/ping")
def read_root():
    return {"Status": "Running !!"}


class LogRequest(BaseModel):
    log: str
@app.post("/verifylog")
def verifylog(payload: LogRequest):
    logText = payload.log
    return analyseLog(logText)


import nest_asyncio
from pyngrok import ngrok
import uvicorn

port = 8000

# Set the Ngrok auth token
ngrok.set_auth_token(Ngrok_token)

# Open a Ngrok tunnel with the specified domain
tunnel = ngrok.connect(port, hostname=Ngrok_domain)

print('Public URL:', tunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=8000)

